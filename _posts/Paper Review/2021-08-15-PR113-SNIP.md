---
title:  "[PR113] SNIP - ICLR 2019"
excerpt: "SNIP: Single-shot network pruning based on connection sensitivity, ICLR 2019"
categories:
  - Computer vision
tags:
  - Pruning
  - Paper Review
  - PR113

use_math: true
---

### Resource

- Title: SNIP: Single-shot network pruning based on connection sensitivity, ICLR 2019
- Paper: [https://arxiv.org/abs/1810.02340](https://arxiv.org/abs/1810.02340)
- Open Review: [https://openreview.net/forum?id=B1VZqjAcYX](https://openreview.net/forum?id=B1VZqjAcYX)


### 1. 기존 pruning 기법의 한계

- 보통 pruning 방법은 미리 학습한 network를 pruning하고 이를 다시 학습하는 과정을 반복하는 iterative한 방법이 주로 사용되어 반복적인 fine-tuning이 필요함.
- 대부분의 pruning 기법이 FCN, CNN 등 architecture에 의존성이 있으며, 또한 pruning 과정에 사용되는 hyper-parameter가 많이 사용하는데 이를 구하는 과정이 휴리스틱한 경우가 많다.

### 2. One-Shot Pruning Method

- 본 논문에서는 training 전에 한번의 pruning을 수행하여 parameter를 줄이는 방법을 제안한다.
- Network의 각 element의 삭제 여부를 나타내는 auxiliary indicator variance  $c\in\{0,1\}^m$를 정의하고 loss function을 다음과 같이 정의함.

\begin{aligned}
\min_{\mathbf{c}, \mathbf{w}}L(\mathbf{c}\odot \mathbf{w};\mathcal{D})&=\min \frac{1}{n}\sum_{i=1}^{n}\mathcal{l}(\mathbf{c}\odot \mathbf{w};(\mathbf{x}_i,\mathbf{y}_i)) \\
\text{s.t.} \, \mathbf{w} &\in \mathbb{R}^m, \\
\mathbf{c} &\in \{0,1\}^m, \, ||\mathbf{c}||_0 \leq \mathcal{k},

\end{aligned}

- parameter $c$로 인해 학습해야할 parameter가 2배가 되어 바로 optimize가 어려워짐. 하지만 pruning 여부, 즉 $c$에 따른 성능 변화에 대해서 loss를 정의하면 weight와 무관하게 최적화 할 수 있음.

$$\begin{aligned}\Delta L_j(\mathbf{w};\mathcal{D})&=L(1\odot \mathbf{w};\mathcal{D})-L((1-\mathbf{e}_j)\odot \mathbf{w};\mathcal{D}) \\ &\approx g_i(\mathbf{w},\mathcal{D})\end{aligned}$$

- (중간과정 생략) weight에 dependency가 적고 한번의 forward pass로 모든 connection을 평가할 수 있는  "connection sensitivity"를 정의함.

$$\begin{aligned}s_j=\frac{|g_j(\mathbf{w};\mathcal{D})|}{\sum_{k=1}^{m}|g_k(\mathbf{w};\mathcal{D})|}\end{aligned}$$

### 3. Experimental Result

- 다음과 같은 순서로 pruning을 수행함.

    1. network의 parameter를 초기화

    2. mini-batch sampling $\mathcal{D}^b$

    3. Connection sensitivity를 계산 $s_j\forall j\in\{1,...,m\}$

    4. Top-$\mathcal{k}$의 parameter만 남기고 pruning

    5. pruned network를 학습

- 다른 pruning 기법 대비 간단한 방법으로 좋은 성능을 나타냄.

![exp1](https://jerogar.github.io/img/PR113/SINP/exp1.png)

- random label을 적용한 결과 pruned network의 경우 loss가 감소하지 않음. 
→ network의 memorization 문제를 방지할 수 있음

![exp2](https://jerogar.github.io/img/PR113/SINP/exp2.png)
