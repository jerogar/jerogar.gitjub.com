---
title:  "[PR113] AdderNet"

categories:
  - Computer vision
tags:
  - Efficient Network
  - Paper Review
  - PR113
---

### Resource

- Title: AdderNet: Do We Really Need Multiplications in Deep Learning? (CVPR 2020)
- Paper: [https://arxiv.org/abs/1912.13200](https://arxiv.org/abs/1912.13200)
- Code: [https://github.com/huawei-noah/AdderNet](https://github.com/huawei-noah/AdderNet)


### 1. Convolution 연산을 $l_1$distance로 변경

- CNN의 convolution 연산은 필터와 입력간의 similarity를 측정하는 연산으로 해석할 수 있으며, float point value의 곱셈으로 구성된 convolution의 연산량 문제를 해결하기 위해 add/sub 연산으로 구성된 similarity measure인  $l_1$ distance로 대체함.

    $$Y(m,n,t)=-\sum_{i=0}^{d}\sum_{j=0}^{d}\sum_{k=0}^{c_{in}}|X(m+i, n+j,k)-F(i,j,k,t)|$$

### 2. $l_1$distance로 구성된 network를 위한 back-propagation 방법 제안

- output feature에 대한 gradient는 Sign 함수로 정의되므로 network의 optimization을 위해 signSGD를 적용해야 하며, 학습이 불안정해지는 문제가 있으므로, $l_2$ norm을 적용하여 full-precision gradient를 활용함.

    $$\frac{\partial Y(m,n,t)}{\partial F(i,j,k,t)}=X(m+i,n+j,k)-F(i,j,k,t)$$

- Input $X$에 대한 gradient는 gradient exploding을 방지하기 위해 Hard Tanh activation을 적용

    $$\frac{\partial Y(m,n,t)}{\partial X(m+i,n+j,k)}=\text{HT}(F(i,j,k,t)-X(m+i,n+j,k))$$

- AdderNet의 output gradient는 CNN 대비 높아서 학습 속도가 느리므로, global/local learning rate와 filter의 gradient를 고려한 adaptive learning rate 기법을 적용하여 개선

    $$\Delta F_l=\gamma \times \alpha_l \times \Delta L(F_l)$$

### 3. Experimental Result

- BNN 대비 월등히 좋은 성능을 보이며, CNN과 유사한 Accuracy를 가짐을 확인.

![AdderNet_Result](https://jerogar.github.io/img/PR113/AdderNet/AdderNet_Perf.png)

- Full precision gradient (FP)와 Adeptive Learning Rate (ALR) 기법을 적용한 경우가 수렴속도가 빠르고 loss가 가장 적음을 확인.

![AdderNet_LearningCurve](https://jerogar.github.io/img/PR113/AdderNet/AdderNet_Curve.png)
